{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some path constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT = r\"..\"\n",
    "DATA_IN = os.path.join(ROOT, \"data\") #Config.get(\"path.data.in\"))\n",
    "DATA_PROCESSED = os.path.join(ROOT, \".processed\") #Config.get(\"path.data.processed\"))\n",
    "DATA_FOLDER = \"smartphone-activity\"\n",
    "SOURCE_DATASET = os.path.join(DATA_IN, DATA_FOLDER ,\"dataset.csv\")\n",
    "SOURCE_ATTRIBUTES = os.path.join(DATA_IN, DATA_FOLDER ,\"attributes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data\n",
    "First we need to read in the dataset and attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example: Reading CSV file without mentioning schema\") \\\n",
    "    .getOrCreate()\n",
    "dataset = spark.read.load(SOURCE_DATASET, format=\"csv\", sep=\",\", inferSchema=True, header=True)\n",
    "dataset.show(10)\n",
    "attrs = spark.read.csv(SOURCE_ATTRIBUTES, header=True)\n",
    "attrs.show(10)\n",
    "\n",
    "print(\"Columns: %s\" % attrs.select(\"name\").count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have a lot of features so lets look for correlations and see if we can remove any of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_correlation(data, threshold=0.5, remove_negative=False):\n",
    "    \"\"\"\n",
    "    Given a numeric pd.DataFrame, this will find highly correlated features,\n",
    "    and return a list of features to remove.\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        DataFrame\n",
    "    threshold : float\n",
    "        correlation threshold, will remove one of pairs of features with a\n",
    "        correlation greater than this value.\n",
    "    remove_negative: Boolean\n",
    "        If true then features which are highly negatively correlated will\n",
    "        also be returned for removal.\n",
    "    Returns\n",
    "    --------\n",
    "    select_flat : list\n",
    "        listof column names to be removed\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    corr_mat = data.corr()\n",
    "    if remove_negative:\n",
    "        corr_mat = np.abs(corr_mat)\n",
    "    corr_mat.loc[:, :] = np.tril(corr_mat, k=-1)\n",
    "    already_in = set()\n",
    "    result = []\n",
    "    for col in corr_mat:\n",
    "        perfect_corr = corr_mat[col][corr_mat[col] > threshold].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            result.append(perfect_corr)\n",
    "    select_nested = [f[1:] for f in result]\n",
    "    select_flat = [i for j in select_nested for i in j]\n",
    "    return set(select_flat)\n",
    "\n",
    "def reduce_feature_set(df=dataset):\n",
    "    pandas_df = df.toPandas()\n",
    "    print(\"All Features: %s\" % len(pandas_df.columns))\n",
    "    reduced_feature_set = find_correlation(pandas_df)\n",
    "    print(\"Reduced Features: %s\" % len(reduced_feature_set))\n",
    "    return list(reduced_feature_set)\n",
    "\n",
    "# reduce_feature_set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we group all the features together"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "VECTOR_COL = \"features\"\n",
    "\n",
    "def create_df_vector(df=dataset, columns: list = reduce_feature_set()):\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=columns,\n",
    "        outputCol=VECTOR_COL)\n",
    "\n",
    "    df_vector = assembler.transform(df)\n",
    "    df_vector = df_vector.select(*[VECTOR_COL, \"activity\"])\n",
    "    return df_vector\n",
    "\n",
    "create_df_vector().show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next lets create our test/training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_training_and_test_data():\n",
    "    df = create_df_vector(dataset).toPandas()\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "    train = spark.createDataFrame(df[msk])\n",
    "    # training_data = spark.createDataFrame(training_data)\n",
    "    # training_data = create_df_vector(training_data)\n",
    "\n",
    "    test = spark.createDataFrame(df[~msk])\n",
    "    # test_data = spark.createDataFrame(test_data)\n",
    "    # test_data = create_df_vector(test_data)\n",
    "\n",
    "    print(\"Training: %s \" % train.count())\n",
    "    print(\"Test Data: %s \" % test.count())\n",
    "    return train, test\n",
    "\n",
    "# create_training_and_test_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Let's train our model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_data, test_data = create_training_and_test_data()\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, featuresCol=\"features\", labelCol=\"activity\")\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since model1 is a Model (i.e., a transformer produced by an Estimator), we can view the parameters it used during fit().\n",
    "This prints the parameter (name: value) pairs, where names are unique IDs for this LogisticRegression instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Model 1 was fit using parameters: \")\n",
    "model1.extractParamMap()\n",
    "\n",
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "# Specify multiple Params.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "# Change output column name\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # type: ignore\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)  # type: ignore\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training_data, paramMapCombined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "model2.extractParamMap()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "Let's test our model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "prediction = model2.transform(test_data)\n",
    "result = prediction.select(\"features\", \"activity\", \"myProbability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "right = wrong = 0\n",
    "for row in result:\n",
    "    if row.activity == row.prediction:\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "\n",
    "total = right + wrong\n",
    "percent = round(right * 100 / total)\n",
    "print(\"Accuracy: %s/%s (%s%%)\" % (right , total, percent))\n",
    "# for row in result:\n",
    "#     print(\"features=%s, activity=%s -> prob=%s, prediction=%s\"\n",
    "#           % (row.features, row.activity, row.myProbability, row.prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MODEL1_PATH = os.path.join(\"..\", \"data\", \"models\", DATA_FOLDER, \"lrm1.model\")\n",
    "MODEL2_PATH = os.path.join(\"..\", \"data\", \"models\", DATA_FOLDER, \"lrm4.model\")\n",
    "\n",
    "# model1.save(MODEL1_PATH)\n",
    "model2.save(MODEL2_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}